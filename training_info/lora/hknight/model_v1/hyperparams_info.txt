+-------------------------------+-----------------------------------------------------+
| Hyperparameter                | Value                                               |
+-------------------------------+-----------------------------------------------------+
| v2                            | False                                               |
| v_parameterization            | False                                               |
| project_name                  | hknight_lora_v1                                     |
| pretrained_model_name_or_path | /content/pretrained_model/Anything-v3-1.safetensors |
| vae                           | /content/vae/anime.vae.pt                           |
| train_folder_directory        | /content/LoRA/train_data                            |
| reg_folder_directory          | /content/LoRA/reg_data                              |
| output_dir                    | /content/LoRA/output                                |
| resume_path                   | False                                               |
+-------------------------------+-----------------------------------------------------+
+--------------------------------+-------+
| Hyperparameter                 | Value |
+--------------------------------+-------+
| enable_bucket                  | True  |
| resolution                     | 512   |
| min_bucket_reso                | 256   |
| max_bucket_reso                | 1024  |
| bucket_reso_steps              | 64    |
| bucket_no_upscale              | False |
| cache_latents                  | True  |
| flip_aug                       | False |
| caption_extension              | .txt  |
| caption_dropout_rate           | 0     |
| caption_dropout_every_n_epochs | 0     |
+--------------------------------+-------+
+-------------------+---------------+
| Hyperparameter    | Value         |
+-------------------+---------------+
| network_dim       | 4             |
| network_alpha     | 1             |
| network_module    | networks.lora |
| network_weight    | False         |
| optimizer_type    | AdamW8bit     |
| optimizer_args    | False         |
| unet_lr           | 0.0001        |
| text_encoder_lr   | 5e-05         |
| lr_scheduler      | constant      |
| lr_warmup_steps   | 0             |
| lr_scheduler_args | 1             |
+-------------------+---------------+
+-----------------------------+------------------------------+
| Hyperparameter              | Value                        |
+-----------------------------+------------------------------+
| prior_loss_weight           | 1.0                          |
| lowram                      | True                         |
| noise_offset                | 0                            |
| max_train_type              | max_train_epochs             |
| max_train_type_value        | 20                           |
| train_batch_size            | 6                            |
| mixed_precision             | fp16                         |
| save_precision              | fp16                         |
| save_n_epochs_type          | save_n_epoch_ratio           |
| save_n_epochs_type_value    | 3                            |
| save_model_as               | safetensors                  |
| max_token_length            | 225                          |
| clip_skip                   | 2                            |
| gradient_checkpointing      | False                        |
| gradient_accumulation_steps | 1                            |
| seed                        | -1                           |
| logging_dir                 | /content/LoRA/logs           |
| additional_argument         | --shuffle_caption --xformers |
+-----------------------------+------------------------------+